---
title: "text analysis whirlwind"
author: "Sasha Lavrentovich"
output:
  html_document:
    df_print: paged
---

```{r}
library(tidyverse) # following the tidy way of things
library(tidytext) # package for text analysis 
library(tm) # functions for text mining such as creating corpora
library(textfeatures) # extracting text features for modelling
library(topicmodels) # for making LDA models 
```

### Reading in Files 
Create a path to the directory with the .txt files

```{r}
ZHfolder <- "/home/sasha/Desktop/Rsessions-UFsummer18/data/zh_small"
SPfolder <- "/home/sasha/Desktop/Rsessions-UFsummer18/data/sp_small"

# see how many files are in the ZH folder 
length(list.files(ZHfolder))
```

### Corpus object 

Volatile because corpus destroyed if destroyed in R
Can apply tm transformations to corpus, now tidytext handles this
Creating a corpus object is a good gateway for creating other other objects such as a term-document matrix or tidy object based on your analysis needs 

```{r eval = FALSE}
# list of corpus 
ZHcorpus <- VCorpus(DirSource(ZHfolder))
SPcorpus <- VCorpus(DirSource(SPfolder))

# access as you would any index 
inspect(ZHcorpus[2])

#view the full text 
writeLines(as.character(ZHcorpus[2]))

clean <- tm_map(ZHcorpus, stripWhitespace)
clean <- tm_map(ZHcorpus, content_transformer(tolower)
clean <- tm_map(ZHcorpus, stemDocument)

writeLines(as.character(clean[2]))

# combine the two corpora 
Vcorpus <- c(ZHcorpus, SPcorpus)
inspect(Vcorpus)
```


### Tidy format with tidytext 

tidy() creates a one row per document table 

```{r eval = FALSE}

ZH_tidy <- tidy(ZHcorpus) %>% 
  mutate(author = "zh")
SP_tidy <- tidy(SPcorpus)  %>% 
  mutate(author = "sp")

corpus <- bind_rows(ZH_tidy, SP_tidy)
corpus
```

unnest_tokens() creates one token per row, stripped of punctuation, and in lower case 

```{r}
corpus %>%
 select(author, id, language, text) %>% 
  unnest_tokens(word, text)

corpus %>%
  select(author, id, language, text) %>% 
  unnest_tokens(sentence, text, token = "sentences")

corpus %>%
  select(author, id, language, text) %>% 
  unnest_tokens(ngram, text, token = "ngrams", n = 2)

corpus %>%
  select(author, id, language, text) %>% 
  unnest_tokens(line, text, token = "regex", pattern = "\n")
```

Exercise: create a new corpus called "tidy_corpus" that is tokenized by words; select to preserve the id, language, text, author columns only in the new tidy_corpus 

```{r}
tidy_corpus <- corpus %>%
  select(id, language, text, author) %>% 
  unnest_tokens(word, text) 

tidy_corpus
```

### Frequencies 
With a tidy corpus, we can apply tidyverse functions such as dplyr's count() that calls groupby before then ungroup after;tally & count are mutate & summarise; so count() says group-by then tally something 

```{r}
tidy_corpus %>% 
  count(word, sort = TRUE) 

```

Remove stop words: stop_words contains data from three lexicons or we could create our own, anti_join looks for matches and identifies and retains whatever is NOT matched 

```{r}
stop_words

lex_corpus <- tidy_corpus %>%
  anti_join(stop_words) %>% 
  filter(!str_detect(word, "[0-9]"))

lex_corpus  %>% 
  count(word, sort = TRUE)
```
Exercise: perform a frequency count of the words without stop words but grouped by author 

```{r}
lex_corpus %>% 
  group_by(author) %>% 
  count(word, sort = TRUE) 

lex_corpus %>% 
  count(author, word, sort = TRUE) 

```

## Visualize frequency 

See the top 15 words across the whole corpus 

```{r}
lex_corpus %>% 
  count(word, sort = TRUE) %>% 
  top_n(15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = word, y = n)) + 
  geom_col() +
  coord_flip()
```

Exercise: visualize the top 10 most frequent words by spanish and chinese authors 
Hint: use top_n(), group_by(), ungroup(), fill argument, facet_wrap()

```{r}
lex_corpus %>% 
  group_by(author) %>% 
  count(word, sort = TRUE) %>% 
  top_n(10) %>%
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  #ungroup() %>% 
  ggplot(aes(word, n, fill = author)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~author, scales = "free_y") + 
  labs(x = NULL, y = "Frequency") + 
  coord_flip()
```

### Term frequency 

See percent of word use across all the docs: term frequency  
```{r}
percent <- lex_corpus %>% 
  count(word) %>% 
  transmute(word, all_words = n / sum(n))

percent
```

Percent word use in each author folder 

```{r}
freq <- lex_corpus %>% 
  count(author, word) %>% 
  mutate(author_words = n / sum(n)) %>% 
  left_join(percent) %>%
  arrange(desc(author_words)) %>% 
  ungroup()

freq
```

Nice visual for comparing word frequences from Silge & Robinson (might need library(scales)). Words that are close to the line have similar frequencies in both language texts so both folders use 'people'. Words that are far from the line are found more in one folder than another. Words above the line are common across the whole folder but not within that specific folder, and those below the line are common in that folder but not across all folders. So 'car' used in spanish but not used in chinese.  
```{r}
ggplot(freq, aes(x = author_words, y = all_words, color = abs(all_words - author_words))) +
        geom_abline(color = "gray40", lty = 2) +
        geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
        geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
        scale_x_log10(labels = scales::percent_format()) +
        scale_y_log10(labels = scales::percent_format()) +
        scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
        facet_wrap(~ author) +
        theme(legend.position="none")
```

How correlated are word frequencies between the authors and the texts? They are all pretty similar... 

```{r}
freq %>% 
  group_by(author) %>% 
  summarize(correlation = cor(author_words, all_words), 
            p_value = cor.test(author_words, all_words)$p.value)
```

### term frequency * inverse document frequency (TF-IDF)

TF-IDF measures the word's frequency adjusted to how rarely it used across docs to see how uniquely important it is for one document within a corpus, see [link](https://www.tidytextmining.com/tfidf.html) by Julia Silge & David Robinson 

 tf-idf(): 0 for more common words because they apepar across the corpora; tf-idf is higher for words that occur in fewer docs in the corpus and may be more unique

```{r}

tf_words <- tidy_corpus %>% 
  count(author, word, sort = TRUE) %>% 
  ungroup()

tf_idf <- tf_words %>% 
  bind_tf_idf(word, author, n) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  mutate(author = factor(author, levels = c("zh", "sp")))

tf_idf
```

Take a look 

```{r}
tf_idf %>% 
  group_by(author) %>% 
  top_n(10, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = author)) + 
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~author, ncol = 2, scales = "free") + 
  coord_flip()
```

## Sentiment Analysis 

Three lexicons that classify feelings:
AFINN: scored -5, 5 
bing: positive vs negative
nrc: yes or no in pos, neg, angry, anticipate, disgust, fear, joy, sad, surpr, trust

```{r}
#get_sentiments("nrc")

sentiments <- tidy_corpus %>% 
  inner_join(get_sentiments("bing")) 
sentiments
```


```{r}
sentiments %>% 
  count(sentiment, word) %>% 
  filter(n >= 10) %>% 
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

```
Not enough words across corpora so you have these gaps   
  
```{r}
sentiments %>% 
  group_by(author) %>% # add this
  count(sentiment, word) %>% 
  filter(n >= 10) %>% 
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  facet_wrap(~author) + # add this
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")
```


### Tagging 

For today, we will make do with a builtin POS dataset. Opetimally, we want to use packages (openNLP, cleanNLP) that are more robust/trained on sentences, but these require python or java backends, spacy or coreNLP so we'll have to settle with more simple tagging. Get these packages though for future work. 

We will make do with a built in POS dataset 
```{r}
parts_of_speech

pos_corpus <- tidy_corpus %>% 
  group_by(author) %>%
  inner_join(parts_of_speech) %>% 
  count(pos) %>% 
  mutate(prop = (n/sum(n))*100)

pos_corpus
```


### Document Term Matrix 

Used for modelling, in a dtm each row is one document, each column is a term, each value is the number of appearances of that term in that document. Terms are usually words, but could be other. 

dtm: contains documents with so many terms, is 95% sparse of (95% of the doc-word pairs are zero)

```{r}
dtm <- DocumentTermMatrix(Vcorpus, 
                          control = list(stopwords = TRUE, 
                                         removePunctuation = TRUE, 
                                         stem = TRUE, 
                                         stripWhitespace = TRUE, 
                                         removeSeparators = TRUE))
dtm
Terms(dtm)

# find terms that occur at least five tiems

findFreqTerms(dtm, 15) 

# find terms that correlate for the term 'people' at at least 0.8

findAssocs(dtm, "people", 0.5)
```


## Topic modelling 

```{r}

lda_model <- LDA(dtm, k = 2, control = list(seed = 1234))


get_terms(lda_model, 10)

# per topc per word probabilities
# one topic per term per row format; for each combination the model finds the probability of that term being generated from that topic; so 'agreed' has a 0.000636 time probability of being generated from topic 1 and a much tinier probability for topic 2 

topics <- tidy(lda_model, matrix = "beta")
topics

# find the ten terms most common within each topic 

top_terms <- topics %>%
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)
top_terms

top_terms %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + 
  coord_flip()

```



  # extra stuff 
  
  See total words for each corpus 
```{r eval=FALSE}

total_words <- tf_words %>% 
  group_by(author) %>% 
  summarize(total = sum(n))
total_words
```
  

  
  
  R text workflow: 

1. Load files into a VCorpus(DirSource(folder-to-text) list 
2. using tm_map, strip stopwords, whitespace, punctuation, etc 
3. convert the corpus object into a document-term-matrix 4. to do some summary stats 
convert dtm to tidytext format to do more with tidy tools
Problems: tm likes dtm, others like corpus, tidy stuff obviously tidy formats and it gets annoying casting one from another

### data frame 
Read the texts from a given directory; fileids and texts are columns
Could use unnest() to parse each text into sentences 

```{r eval = FALSE}
df <- data_frame(file = dir(SPfolder, full.names = TRUE)) %>% 
  mutate(text = map(file, read_lines)) %>% 
  transmute(doc = basename(file), text)
df

```



## Resources 
List of R NLP packages/resources in R: [link](https://www.r-pkg.org/ctv/NaturalLanguageProcessing)
